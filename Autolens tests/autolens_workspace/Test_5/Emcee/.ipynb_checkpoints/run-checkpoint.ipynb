{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Attention!\n",
    "### This code runs in MPI mode.\n",
    "\n",
    "\n",
    "Trying to recover the input values of the simulation. The free parameters are:\n",
    "   - One ML, One beta, qinc, mbh, kappa_s, qDm, mag_shear, phi_shear and gamma.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Control time packages\n",
    "import time\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "#MPI\n",
    "from schwimmbad import MPIPool\n",
    "\n",
    "#General packages\n",
    "import numpy as np\n",
    "from My_Jampy import JAM\n",
    "import emcee\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Constants and usefull packages\n",
    "from astropy.cosmology import Planck15 as cosmo\n",
    "from astropy.cosmology import z_at_value\n",
    "from astropy.constants import G, M_sun, c\n",
    "import astropy.units as u\n",
    "\n",
    "#Autolens Model packages\n",
    "import autolens as al\n",
    "import autolens.plot as aplt\n",
    "\n",
    "#Combined Model package\n",
    "import CombinedModel\n",
    "\n",
    "data_folder = \"/home/carlos/Documents/GitHub/Master-Degree/Autolens tests/autolens_workspace/Test_4/Simulation_Data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we use the MPI\n",
    "with MPIPool() as pool:\n",
    "\n",
    "    if not pool.is_master():\n",
    "        pool.wait()\n",
    "        sys.exit(0)\n",
    "        \n",
    "        #Reading MGE inputs\n",
    "    surf_lum, sigma_lum, qobs_lum = np.loadtxt(\"Input/JAM_Input.txt\", unpack=True)      #MGE decomposition\n",
    "    surf_dm, sigma_dm , qobs_dm   = np.loadtxt(\"Input/eNFW.txt\", unpack=True)           #DM component\n",
    "    norm_psf, sigma_psf           = np.loadtxt(\"Input/MUSE_Psf_model.txt\", unpack=True) #PSF\n",
    "    x, y, vrms, erms              = np.loadtxt(\"Input/vrms_data.txt\", unpack=True)      #vrms data\n",
    "    \n",
    "    pixsize = 0.2                                                           #MUSE pixel size\n",
    "    z_l     = 0.299                                                         #Lens Redshift\n",
    "    z_s     = 3.100                                                         #Source Redshift \n",
    "    D_l     = cosmo.angular_diameter_distance(z_l).value                    #Distance to Lens [Mpc] \n",
    "    \n",
    "    ## Models inicialization\n",
    "\n",
    "    \"\"\"\n",
    "        To inicialize the model, we set some random values for the parameters. But it's only necessary for initialize the model. During the non-linear search, this values will be updated constantly until the best fit.\n",
    "    \"\"\"  \n",
    "    mbh     = 1e9                                                           #mass of black hole [log10(M_sun)]\n",
    "    beta    = np.full_like(surf_lum, 0.35)                                  #anisotropy [ad]\n",
    "    inc     = 75                                                            #inclination [deg]\n",
    "    inc_rad = np.radians(inc)\n",
    "    qinc    = np.sqrt(np.min(qobs_lum)**2 - \n",
    "                        (1 - np.min(qobs_lum)**2)/np.tan(inc_rad)**2)       #Deprojected axial ratio for inclination\n",
    "    qDM     = np.sqrt( qobs_dm[0]**2 - np.cos(inc_rad)**2)/np.sin(inc_rad)  #Deprojected DM axial ratio\n",
    "    kappa_s = 0.075                                                         #kappa_s of DM profile\n",
    "    r_s     = 11.5                                                          #Scale radius of DM [arcsec]\n",
    "    ml      = 5.00                                                          #mass to light ratio\n",
    "    shear_comp = al.convert.shear_elliptical_comps_from(magnitude=0.02, phi=88) #external shear\n",
    "    \n",
    "    #Autolens Data\n",
    "    imaging = al.Imaging.from_fits(\n",
    "            image_path=f\"{data_folder}/arcs_simulation.fits\",\n",
    "            noise_map_path=f\"{data_folder}/noise_simulation.fits\",\n",
    "            psf_path=f\"{data_folder}/psf_simulation.fits\",\n",
    "            pixel_scales=0.1,\n",
    "        )\n",
    "\n",
    "    mask = al.Mask.circular_annular(centre=(0.0, 0.), inner_radius=2.1, outer_radius=4.1,\n",
    "                                  pixel_scales=imaging.pixel_scales, shape_2d=imaging.shape_2d) #Create a mask\n",
    "\n",
    "    masked_image = al.MaskedImaging(imaging=imaging, mask=mask, inversion_uses_border=True)     #Masked image\n",
    "    #--------------------------------------------------------------------------------------------------#\n",
    "    # JAMPY MODEL\n",
    "    #Now we start our Jampy class\n",
    "    Jam_model = JAM(ybin=y*pixsize, xbin=x*pixsize, inc=inc, distance=D_l, mbh=mbh, beta=beta, rms=vrms, erms=erms,\n",
    "                       normpsf=norm_psf, sigmapsf=sigma_psf*pixsize, pixsize=pixsize)\n",
    "\n",
    "    #Add Luminosity component\n",
    "    Jam_model.luminosity_component(surf_lum=surf_lum, sigma_lum=sigma_lum,\n",
    "                                        qobs_lum=qobs_lum, ml=ml)\n",
    "    #Add DM component\n",
    "    Jam_model.DM_component(surf_dm=kappa_s * surf_dm, sigma_dm=sigma_dm, qobs_dm=qobs_dm)\n",
    "    \n",
    "    #--------------------------------------------------------------------------------------------------#\n",
    "    # PYAUTOLENS MODEL\n",
    "    #MGE mass profile\n",
    "    mass_profile = al.mp.MGE()\n",
    "    ell_comps    = al.convert.elliptical_comps_from(axis_ratio=qobs_dm[0], phi=0.0) #Elliptical components in Pyautolens units\n",
    "    eNFW         = al.mp.dark_mass_profiles.EllipticalNFW(kappa_s=kappa_s, elliptical_comps=ell_comps ,scale_radius=r_s) #Analytical eNFW profile\n",
    "\n",
    "\n",
    "    #Components\n",
    "    #Do not include MGE DM component here\n",
    "    mass_profile.MGE_comps(z_l=z_l, z_s=z_s, \n",
    "                           surf_lum=surf_lum, sigma_lum=sigma_lum, qobs_lum=qobs_lum, ml=ml, mbh=mbh) \n",
    "    mass_profile.Analytic_Model(eNFW)  #Include Analytical NFW\n",
    "    #--------------------------------------------------------------------------------------------------#\n",
    "    # COMBINED MODEL\n",
    "\n",
    "        #Just remembering, by default the model does not include dark matter.\n",
    "    model = CombinedModel.Models(Jampy_model=Jam_model, mass_profile=mass_profile,\n",
    "                                 masked_imaging=masked_image, quiet=False)\n",
    "\n",
    "    model.mass_to_light(ml_kind='scalar')               #Setting gradient ML\n",
    "    model.beta(beta_kind='scalar')                      #Seting vector anisotropy\n",
    "    model.has_MGE_DM(a=True, filename=\"Input/eNFW.txt\", include_MGE_DM=\"Dynamical\") #Setting Dark matter component\n",
    "    #--------------------------------------------------------------------------------------------------#\n",
    "    #  EMCEE\n",
    "    \"\"\"\n",
    "        Pay close attention to the order in which the components are added. \n",
    "        They must follow the log_probability unpacking order.\n",
    "    \"\"\"\n",
    "\n",
    "    #In order: ML, beta, qinc, log_mbh, kappa_s, qDM, mag_shear, phi_shear, gamma\n",
    "    nwalkers = 120                                                  #Number of walkers\n",
    "    #We distribute the initial position of walkers using a uniform distribution over all the possible values.\n",
    "    pos = np.random.uniform(low=[0.5, -3, 0.1, 6, 0, 0.5, 0, 10, 0.8], high=[10, 3, 0.50, 10, 1, 1, 0.1, 150, 1.2], size=[nwalkers, 9])\n",
    "    nwalkers, ndim = pos.shape                                      #Number of walkers/dimensions\n",
    "    \n",
    "    \"\"\"\n",
    "        We save the results in a table.\n",
    "        This table marks the number of iterations, the mean acceptance fraction,the running time, and the mean accep. fraction of last 100 its. \n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    np.savetxt('Output_LogFile.txt', np.column_stack([0, 0, 0, 0]),\n",
    "                                fmt=b'\t%i\t %e\t\t\t %e     %e', \n",
    "                                header=\"Output table for the combined model: Dynamic.\\n Iteration\t Mean acceptance fraction\t Processing Time    Last 100 Mean Accp.\")\n",
    "    #Print the number os cores/workers\n",
    "    print(\"Workers nesse job:\", pool.workers)\n",
    "    print(\"Start\")\n",
    "    \n",
    "    ml_model     = np.array([ml])                      #mass to light ratio\n",
    "    beta_model   = np.array([beta[0]])  #anisotropy [ad]\n",
    "    p0           = np.append(ml_model, beta_model)\n",
    "    others = np.array([qinc, np.log10(mbh), kappa_s, qDM, 0.02, 88, 1.0])#Other parameters\n",
    "    p0     = np.append(p0, others)                        #All parameters\n",
    "    \n",
    "    model(p0)\n",
    "\n",
    "    #Backup\n",
    "    filename = \"Simulation4.h5\"\n",
    "    backend = emcee.backends.HDFBackend(filename)\n",
    "    backend.reset(nwalkers, ndim)\n",
    "    moves=[(emcee.moves.DEMove(), 0.80), (emcee.moves.DEMove(gamma0=1.0), 0.20)]\n",
    "\n",
    "    #Initialize the sampler\n",
    "    sampler = emcee.EnsembleSampler(nwalkers, ndim, model, pool=pool,\n",
    "                                     backend=backend, moves=moves)\n",
    "    \n",
    "    #Burn in fase\n",
    "    burnin = 1                           #Number of burn in steps\n",
    "    print(\"Burn in with %i steps\"%burnin)\n",
    "    start = time.time()\n",
    "    state = sampler.run_mcmc(pos, nsteps=burnin, progress=True)\n",
    "    print(\"\\n\")\n",
    "    print(\"Burn in elapsed time:\", time.time() - start)\n",
    "    sampler.reset()\n",
    "    print(\"\\n\")\n",
    "    print(\"End of burn-in fase\")\n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"Testing velocity of 1 step with %i walkers.\"%nwalkers)\n",
    "    start = time.time()\n",
    "    state = sampler.run_mcmc(pos, nsteps=burnin, progress=True)\n",
    "    print(\"\\n\")\n",
    "    print(\"Test elapsed time:\", time.time() - start)\n",
    "    sampler.reset()\n",
    "    print(\"\\n\")\n",
    "    #End of burn in fase\n",
    "    \n",
    "    nsteps = 1                          #Number of walkes \n",
    "\n",
    "    # We'll track how the average autocorrelation time estimate changes\n",
    "    index = 0\n",
    "    autocorr = np.empty(nsteps)\n",
    "    # This will be useful to testing convergence\n",
    "    old_tau = np.inf\n",
    "    # This saves how many walkers have been accepted in the last 100 steps\n",
    "    old_accp = np.zeros(nwalkers,)\n",
    "\n",
    "    # Now we'll sample for up to max_n steps\n",
    "    start = time.time()\n",
    "    global_time = time.time()\n",
    "    \n",
    "    for sample in sampler.sample(state, iterations=nsteps, progress=True):\n",
    "        # Only check convergence every 100 steps\n",
    "        if sampler.iteration % 100:\n",
    "            continue\n",
    "        print(\"\\n\")\n",
    "        print(\"##########################\")\n",
    "\n",
    "        #Compute how many walkes have been accepted during the last 100 steps\n",
    "\n",
    "        new_accp = sampler.backend.accepted             #Total number of accepted\n",
    "        old_accp = new_accp - old_accp                  #Number of accepted in the last 100 steps\n",
    "        mean_accp_100 = np.mean(old_accp/float(100))    #Mean accp fraction of last 100 steps\n",
    "\n",
    "        #Update a table output with acceptance\n",
    "        table = np.loadtxt(\"Output_LogFile.txt\")\n",
    "\n",
    "        iteration = sampler.iteration\n",
    "        accept = np.mean(sampler.acceptance_fraction)\n",
    "        total_time = time.time() - global_time\n",
    "        upt = np.column_stack([iteration, accept, total_time, mean_accp_100])\n",
    "\n",
    "        np.savetxt('Output_LogFile.txt', np.vstack([table, upt]),\n",
    "                                fmt=b'\t%i\t %e\t\t\t %e             %e', \n",
    "                            header=\"Output table for the combined model: Dynamic.\\n Iteration\t Mean acceptance fraction\t Processing Time    Last 100 Mean Accp. Fraction\")\n",
    "\n",
    "        # Compute the autocorrelation time so far\n",
    "        # Using tol=0 means that we'll always get an estimate even\n",
    "        # if it isn't trustworthy\n",
    "        tau = sampler.get_autocorr_time(tol=0)\n",
    "        autocorr[index] = np.mean(tau)\n",
    "        index += 1\n",
    "\n",
    "        # Check convergence\n",
    "        converged = np.all(tau * 100 < sampler.iteration)\n",
    "        converged &= np.all(np.abs(old_tau - tau) / tau < 0.01)\n",
    "        if converged:\n",
    "            if 0.2 < accept < 0.35:\n",
    "                break\n",
    "        old_tau = tau\n",
    "\n",
    "\n",
    "\n",
    "    end = time.time()\n",
    "    print('\\n')\n",
    "    print(\"Final\")\n",
    "    multi_time = end - start\n",
    "    print(\"Multiprocessing took {0:.1f} seconds\".format(multi_time))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

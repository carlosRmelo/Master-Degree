{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Carlos Roberto de Melo\n",
    "    \n",
    "Date: 10/02/2020\n",
    "    \n",
    "Obj: Para que o código possa rodar em um cluster (com diferentes nós) é neessário a implementação do module OpenMPI.\n",
    "    Este notebook realiza essa implementação. Em geral, o MPI pede que o arquivo esteja na forma de uma scrip.py, por isso nosso primeiro bloco (f.write) gera esse arquivo .py\n",
    "    Além disso, um pacote destino ao MPI deve ser utilizado (schwimmbad), bem como o próprio OpenMPI ou mpi4py.\n",
    "    \n",
    "O segundo blobo contém a linha de código para executar o scrip.py com a utilização do MPI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"JAM+AUTOLENS_RUN.py\", \"w\") as f:\n",
    "    f.write(\"\"\"\n",
    "\n",
    "#General packages\n",
    "import numpy as np\n",
    "import My_Jampy\n",
    "import emcee\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from time import perf_counter as clock\n",
    "from multiprocessing import Pool\n",
    "from schwimmbad import MPIPool\n",
    "import time\n",
    "import os\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "#Autolens Model packages\n",
    "\n",
    "import autolens as al\n",
    "import autolens.plot as aplt\n",
    "#print(\"Pyautolens version:\", al.__version__)\n",
    "\n",
    "from pyprojroot import here\n",
    "import numpy as np\n",
    "\n",
    "from time import perf_counter as clock\n",
    "\n",
    "from astropy.cosmology import Planck15 as cosmo\n",
    "from astropy.constants import G, M_sun, c\n",
    "import astropy.units as u\n",
    "\n",
    "workspace_path = str(here())\n",
    "#print(\"Workspace Path: \", workspace_path)\n",
    "#------------------------------------------------------------------------------------#\n",
    "\n",
    "## DATA\n",
    "\n",
    "#Lendo os dados de fotometria, DM halo e cinemática\n",
    "surf_star_dat, sigma_star_dat, qstar_dat = np.loadtxt('JAM Input.txt', unpack=True) #Star\n",
    "surf_DM_dat, sigma_DM_dat, qDM_dat = np.loadtxt('pseudo-DM Input.txt', unpack=True) #DM\n",
    "y_px, x_px, vel,  disp, chi, dV, dsigma = np.loadtxt('pPXF DATA.txt', unpack=True)  #pPXF  \n",
    "\n",
    "### Global Constantes\n",
    "\n",
    "#Lens parameters\n",
    "\n",
    "z_lens = 0.035\n",
    "z_source = 2.1\n",
    "\n",
    "D_l = cosmo.angular_diameter_distance(z_lens)\n",
    "D_s = cosmo.angular_diameter_distance(z_source)\n",
    "D_ls = cosmo.angular_diameter_distance_z1z2(z_lens, z_source)\n",
    "\n",
    "#Useful constants\n",
    "metre2Mpc = (1*u.m).to(u.Mpc)/u.m           #Constant factor to convert metre to Mpc.\n",
    "kg2Msun = (1*u.kg/M_sun)*u.solMass/u.kg     #Constant factor to convert kg to Msun\n",
    "\n",
    "G_Mpc = G*(metre2Mpc)**3/kg2Msun            #Gravitational constant in Mpc³/(Msun s²)\n",
    "c_Mpc = c*metre2Mpc                         #Speed of light in Mpc/s\n",
    "\n",
    "### Global Parameters\n",
    "\n",
    "#To inicialize the model, we set an ML igual to 1 for every component in Star MGE.\n",
    "    #But it's only necessary for initialize the model. \n",
    "    #During the non-linear search, this ML will be updated constantly until the best fit.\n",
    "    #Same as above for the Anisotropy parameter.\n",
    "    \n",
    "inc = 120.                                                    #Assumed galaxy inclination                  \n",
    "distance = D_l                                                #Distance in Mpc\n",
    "mbh =  1e8*u.solMass                                          #Mass of SMBH in solar masses\n",
    "beta = np.zeros(surf_star_dat.shape)                          #Anisotropy parameter. One for each gaussian component \n",
    "ML = np.ones(surf_star_dat.shape)*u.solMass/u.solLum          #Mass to light ratio per gaussian in M_sun/L_sun\n",
    "\n",
    "#DM\n",
    "surf_DM_dat = surf_DM_dat*(u.solMass/u.pc**2)                                    #Surface Density in M_sun/pc²\n",
    "sigma_DM_dat_ARC = sigma_DM_dat*u.arcsec                                         #Sigma in arcsec\n",
    "sigma_DM_dat_PC = (sigma_DM_dat_ARC*D_l).to(u.pc, u.dimensionless_angles())      #Convert sigma in arcsec to sigma in pc\n",
    "qDM_dat = qDM_dat                                                                #axial ratio of DM halo\n",
    "\n",
    "#Stars\n",
    "surf_star_dat = surf_star_dat*(u.solLum/u.pc**2)                                #Surface luminosity Density in L_sun/pc²\n",
    "sigma_star_dat_ARC = sigma_star_dat*u.arcsec                                    #Sigma in arcsec\n",
    "sigma_star_dat_PC = (sigma_star_dat_ARC*D_l).to(u.pc, u.dimensionless_angles()) #Convert sigma in arcsec to sigma in pc\n",
    "qstar_dat = qstar_dat                                                           #axial ratio of star photometry\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "# JAMPY MODEL\n",
    "\n",
    "#Definindo algumas quantidades dos instrumentos e características da galáxia\n",
    "\n",
    "sigmapsf = 0.2420                                   #Sigma psf de onde foram coletados os dados de cinemática, em arcsec\n",
    "pixsize = 0.6                                       #pixel scale, em px/arcsec, dos dados de cinemática\n",
    "e = 0.24                                            #elipticidade da galáxia. Valor encontrado pelo find_my_galaxy\n",
    "\n",
    "\n",
    "#Selecionando os pixels onde queremos calcular o modelo\n",
    "\n",
    "x_good = []\n",
    "y_good = []\n",
    "disp_good = []\n",
    "vel_good = []\n",
    "dV_good = []\n",
    "dsigma_good = []\n",
    "\n",
    "for i in range(len(disp)):\n",
    "    r = np.sqrt((x_px[i]*pixsize)**2 + ((y_px[i])*pixsize/(1-e))**2)\n",
    "    if r < 5:\n",
    "        x_good.append(x_px[i])\n",
    "        y_good.append(y_px[i])\n",
    "        disp_good.append(disp[i])\n",
    "        vel_good.append(vel[i])\n",
    "        dV_good.append(dV[i])\n",
    "        dsigma_good.append(dsigma[i])\n",
    "\n",
    "#Calculando a Velocidade Vrms\n",
    "    #Note que primeiro identificamos o px com a maior dispersão de vlocidades, de modo a identificar o centro da\n",
    "    #galáxia. Após isso, calculamos a velocidade de rotação com relação a esse centro. Somente então podemos\n",
    "    #calcular a velocidade Vrms e o erro erms propagado associado.\n",
    "idx_max = np.where(np.array(disp_good) == max(disp_good))\n",
    "\n",
    "vel_good = vel_good - vel_good[idx_max[0][0]]\n",
    "vrms = np.sqrt(np.array(vel_good)**2 + np.array(disp_good)**2) #Vrms velocity\n",
    "erms = np.sqrt((np.array(dV_good)*np.array(vel_good))**2 + (np.array(dsigma_good)*np.array(disp_good))**2)/vrms #error in vrms\n",
    "\n",
    "#Definindo os dados de entrada do modelo dinâmico\n",
    "\n",
    "    #Posição, em arcsec, onde vamos calcular o modelo\n",
    "xbin = np.array(x_good)*pixsize\n",
    "ybin = np.array(y_good)*pixsize\n",
    "\n",
    "r = np.sqrt(xbin**2 + (ybin/(1-e))**2)              #Radius in the plane of the disk\n",
    "rms = vrms                                          #Vrms field in km/s\n",
    "erms = erms                                         #1-sigma erro na dispersão\n",
    "goodBins =    (r > 0)                               #Informa quais valores de r são bons para gerar o modelo.\n",
    "\n",
    "#Inicializando o modelo dinâmico\n",
    "Jampy_Model = My_Jampy.Jam_axi_rms(ybin=ybin, xbin=xbin,beta=beta, mbh=mbh.value, distance=distance.value,\n",
    "                                surf_lum=surf_star_dat.value, sigma_lum=sigma_star_dat_ARC.value, qobs_lum=qstar_dat,\n",
    "                                surf_DM=surf_DM_dat.value, sigma_DM=sigma_DM_dat_ARC.value, qobs_DM=qDM_dat,\n",
    "                                ml=ML.value, goodBins=goodBins, sigmapsf=sigmapsf, rms=rms, erms=erms,\n",
    "                                pixsize=pixsize, inc=inc)\n",
    "                                \n",
    "#--------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "# Pyautolens Model\n",
    "\n",
    "#Convert  surf_DM_dat to total mass per Guassian\n",
    "\n",
    "Mass_DM_dat = 2*np.pi*surf_DM_dat*(sigma_DM_dat_PC**2)*qDM_dat      #Total mass per gaussian component in M_sun\n",
    "\n",
    "#print(\"Total Mass per Gaussian component in DM profile:\")\n",
    "#print(Mass_DM_dat)\n",
    "\n",
    "#Convert surf_star_dat to total Luminosity per Guassian and then to total mass per gaussian\n",
    "\n",
    "Lum_star_dat = 2*np.pi*surf_star_dat*(sigma_star_dat_PC**2)*qstar_dat    #Total luminosity per gaussian component in L_sun\n",
    "\n",
    "#print(\"Total Luminosity per Gaussian component of Stars:\")\n",
    "#print(Lum_star_dat)\n",
    "\n",
    "#Update the stellar mass based on M/L.\n",
    "\n",
    "Mass_star_dat = Lum_star_dat*ML                          #Total star mass per gaussian in M_sun\n",
    "\n",
    "#print(\"Total Mass per Gaussian component of Star:\")\n",
    "#print(Mass_star_dat)\n",
    "\n",
    "#Inserting a Gaussian to represent SMBH at the center of the galaxy\n",
    "\n",
    "sigmaBH_ARC = 0.01*u.arcsec\n",
    "'''\n",
    "        This scalar gives the sigma in arcsec of the Gaussian representing the\n",
    "        central black hole of mass MBH (See Section 3.1.2 of `Cappellari 2008.\n",
    "        <http://adsabs.harvard.edu/abs/2008MNRAS.390...71C>`_)\n",
    "        The gravitational potential is indistinguishable from a point source\n",
    "        for ``radii > 2*RBH``, so the default ``RBH=0.01`` arcsec is appropriate\n",
    "        in most current situations.\n",
    "\n",
    "        ``RBH`` should not be decreased unless actually needed!\n",
    "    '''\n",
    "\n",
    "\n",
    "sigmaBH_PC = (sigmaBH_ARC*D_l).to(u.pc, u.dimensionless_angles())        #Sigma of the SMBH in pc\n",
    "surfBH_PC = mbh/(2*np.pi*sigmaBH_PC**2)                                  #Mass surface density of SMBH\n",
    "qSMBH = 1.                                                               #Assuming a circular gaussian\n",
    "Mass_SMBH_dat = 2*np.pi*surfBH_PC*(sigmaBH_PC**2)*qSMBH                  #SMBH Total mass \n",
    "\n",
    "#print(\"Total Mass of SMBH\")\n",
    "#print(Mass_SMBH_dat)\n",
    "\n",
    "#Defining the general inputs for the model\n",
    "i = np.deg2rad(inc)*u.rad                                                             #Inclination angle in rad\n",
    "Total_Mass = np.concatenate((Mass_star_dat, Mass_DM_dat, Mass_SMBH_dat), axis=None)   #Mass per gaussian component in M_sun\n",
    "Total_q = np.concatenate((qstar_dat, qDM_dat, qSMBH), axis=None)                      #Total axial ratio per gaussian\n",
    "\n",
    "\n",
    "Total_q_proj = np.sqrt(Total_q**2 - np.cos(i)**2)/np.sin(i)                                       #Total projected axial ratio per gaussian\n",
    "Total_sigma_ARC = np.concatenate((sigma_star_dat_ARC, sigma_DM_dat_ARC, sigmaBH_ARC), axis=None)  #Total sigma per gaussian in arcsec\n",
    "Total_sigma_RAD = Total_sigma_ARC.to(u.rad)                                                       #Total sigma per gaussian in radians\n",
    "\n",
    "#print(\"Total Mass per Gaussian of Model:\")\n",
    "#print(Total_Mass)\n",
    "\n",
    "### __Reading simulated data__\n",
    "\n",
    "#Paths\n",
    "dataset_type = \"JAM+Pyautolens\"\n",
    "dataset_name = \"Data\"\n",
    "dataset_path = f\"{workspace_path}/{dataset_type}/{dataset_name}\"\n",
    "\n",
    "#Load data\n",
    "imaging = al.Imaging.from_fits(\n",
    "    image_path=f\"{dataset_path}/arcs_resized.fits\",\n",
    "    noise_map_path=f\"{dataset_path}/noise_map_resized.fits\",\n",
    "    psf_path=f\"{dataset_path}/psf.fits\",\n",
    "    pixel_scales=0.04,\n",
    ")\n",
    "\n",
    "#Load mask\n",
    "mask_custom = al.Mask.from_fits(\n",
    "    file_path=f\"{dataset_path}/mask gui.fits\", hdu=0, pixel_scales=imaging.pixel_scales\n",
    ")\n",
    "\n",
    "masked_imaging = al.MaskedImaging(imaging=imaging, mask=mask_custom)\n",
    "\n",
    "#Plot\n",
    "'''\n",
    "aplt.Imaging.subplot_imaging(\n",
    "    imaging=imaging, mask=mask_custom, include=aplt.Include(border=True)\n",
    ")\n",
    "'''\n",
    "\n",
    "### __Defining the MGE mass model__\n",
    "\n",
    "#Iniciando o modelo MGE para a lente\n",
    "\n",
    "mass_profile = al.mp.MGE(centre=(0.0, 0.0))                         #Definindo o modelo de massa\n",
    "mass_profile.MGE_comps(M=Total_Mass.value, sigma=Total_sigma_RAD.value,\n",
    "                       q=Total_q_proj.value, z_l=z_lens, z_s=z_source)        #Defindo os dados de entrada\n",
    "\n",
    "mass_profile.MGE_Grid_parameters(masked_imaging.grid)               #Criando a grid de parâmetros para o cálculo\n",
    "                                                                         #em paralelo\n",
    "#Criando o modelo da lente\n",
    "lens_galaxy = al.Galaxy(                                            \n",
    "        redshift=0.035,\n",
    "        mass=mass_profile,\n",
    "        shear=al.mp.ExternalShear(elliptical_comps=(0,0)),\n",
    "    )\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------#\n",
    "#------------------------------------------------ EMCEE -----------------------------------------------------#\n",
    "\n",
    "### Priors\n",
    "\n",
    "# parameter boundaries. [lower, upper]\n",
    "boundary = {'inc': [70, 120], 'beta': [-5, 5], 'ml': [0.5, 15], 'log_mbh':[7, 11], 'qDM': [0.15, 1],\n",
    "                'log_rho_s':[6, 13], 'mag_shear': [-0.2, 0.2], 'phi_shear': [-0.2, 0.2],\n",
    "                'gamma': [-2, 2]\n",
    "               }\n",
    "\n",
    "# parameter gaussian priors. [mean, sigma]\n",
    "prior = {'inc': [90, 10], 'beta': [0.0, 2], 'ml': [1.0, 5],'log_mbh':[8, 3], 'qDM': [0.5, 3e-1],\n",
    "            'log_rho_s':[10, 4], 'mag_shear': [0, 0.1], 'phi_shear': [0, 0.1], 'gamma': [1.0, 0.5]\n",
    "            }\n",
    "\n",
    "def check_ML_Grad(ml):\n",
    "    \n",
    "    #Check if the mass-to-light ratio is descending and inside boundaries.\n",
    "    \n",
    "    for i in range(len(ml)):\n",
    "        if boundary['ml'][0] < ml[i] < boundary['ml'][1]:\n",
    "            pass\n",
    "        else:\n",
    "            return -np.inf\n",
    "        \n",
    "    for i in range(len(ml) -1):\n",
    "        if ml[i] >= ml[i+1]:\n",
    "            pass\n",
    "        else:\n",
    "            return -np.inf\n",
    "    \n",
    "    return 0.0\n",
    "\n",
    "def check_boundary(parsDic):\n",
    "    '''\n",
    "    Check whether parameters are within the boundary limits\n",
    "    input\n",
    "      parsDic: parameter dictionary {'paraName', value}\n",
    "    output\n",
    "      -np.inf or 0.0\n",
    "    '''   \n",
    "    #Check if any deprojected axial ratio is too low (q' <=0 or q' <= 0.05) for the dynamical model.\n",
    "    inc = np.radians(parsDic['inc'])\n",
    "        #Stellar\n",
    "    qintr_star = qstar_dat**2 - np.cos(inc)**2\n",
    "    if np.any(qintr_star <= 0):\n",
    "        #print('erro no qstar1')\n",
    "        return -np.inf\n",
    "    \n",
    "    qintr_star = np.sqrt(qintr_star)/np.sin(inc)\n",
    "    if np.any(qintr_star <= 0.05):\n",
    "        #print('erro no qstar')\n",
    "        return -np.inf\n",
    "    \n",
    "        #DM\n",
    "    qintr_DM = parsDic['qDM']**2 - np.cos(inc)**2\n",
    "    if qintr_DM <= 0:\n",
    "        #print('erro no qdm1')\n",
    "        return -np.inf\n",
    "    \n",
    "    qintr_DM = np.sqrt(qintr_DM)/np.sin(inc)\n",
    "    if qintr_DM <= 0.05:\n",
    "        #print('erro no qdm')\n",
    "        return -np.inf\n",
    "    \n",
    "    #Check if ml is gradient and inside boundaries\n",
    "    if not np.isfinite(check_ML_Grad(parsDic['ml'])):\n",
    "        #print('erro ml')\n",
    "        return -np.inf\n",
    "    \n",
    "    #Check if beta is within boundary limits\n",
    "    for i in range(len(parsDic['beta'])):\n",
    "        if boundary['beta'][0] < parsDic['beta'][i] < boundary['beta'][1] :\n",
    "            pass\n",
    "        else:\n",
    "            #print('erro beta')\n",
    "            return -np.inf\n",
    "        \n",
    "    for i in range(len(parsDic['beta'])):\n",
    "        if parsDic['beta'][i] !=1:\n",
    "            pass\n",
    "        else:\n",
    "            return -np.inf\n",
    "\n",
    "    #Check if the others parameters are within the boundary limits\n",
    "    keys = set(parsDic.keys())\n",
    "    excludes = set(['ml', 'beta'])\n",
    "    \n",
    "    \n",
    "    for keys in keys.difference(excludes):\n",
    "        if boundary[keys][0] < parsDic[keys] < boundary[keys][1]:\n",
    "            pass\n",
    "        else:\n",
    "            #print('erro',keys)\n",
    "            return -np.inf\n",
    "    return 0.0\n",
    "    \n",
    "\n",
    "\n",
    "def log_prior(parsDic):\n",
    "    '''\n",
    "    Calculate the gaussian prior lnprob\n",
    "    input\n",
    "      parsDic: parameter dictionary {'paraName', value}\n",
    "    output\n",
    "      lnprob\n",
    "    '''\n",
    "    \n",
    "    rst = 0\n",
    "    \n",
    "    #log_prior for mass-to-light\n",
    "    for i in range(len(parsDic['ml'])):\n",
    "        rst += -0.5 * (parsDic['ml'][i] - prior['ml'][0])**2/prior['ml'][1]**2\n",
    "        \n",
    "    #log_prior for beta\n",
    "    for i in range(len(parsDic['beta'])):\n",
    "        rst += -0.5 * (parsDic['beta'][i] - prior['beta'][0])**2/prior['beta'][1]**2\n",
    "        \n",
    "    #log_prior for others parameters\n",
    "    keys = set(parsDic.keys())\n",
    "    excludes = set(['ml', 'beta'])\n",
    "    for keys in keys.difference(excludes):\n",
    "        rst += -0.5 * (parsDic[keys] - prior[keys][0])**2/prior[keys][1]**2\n",
    "    \n",
    "    return rst\n",
    "        \n",
    "\n",
    "\n",
    "def Updt_Pyautolens(parsDic):\n",
    "    '''\n",
    "    Update the Lens mass model\n",
    "    input\n",
    "      parsDic: parameter dictionary {'paraName', value}\n",
    "    '''\n",
    "    #Inclination\n",
    "    inc_model = np.deg2rad(parsDic['inc'])                          #Get new inclination in radians\n",
    "    \n",
    "    #Stellar parameters\n",
    "    Stellar_Mass_model = (Lum_star_dat*parsDic['ml']).value         #Updt the stellar mass \n",
    "    \n",
    "    #DM parameters\n",
    "    qDM_model = np.ones(qDM_dat.shape)*parsDic['qDM']                                              #Updt DM axial ratio \n",
    "    Mass_DM_model = (10**parsDic['log_rho_s'])*(2*np.pi*surf_DM_dat*(sigma_DM_dat_PC**2)*qDM_model).value    #Updt DM Mass\n",
    "    \n",
    "    \n",
    "    #Total mass and new projected axis here we add the new SMBH mass\n",
    "    Total_Mass_model = np.concatenate((Stellar_Mass_model, Mass_DM_model, 10**parsDic['log_mbh']), axis=None)  #New total mass\n",
    "    Total_q_model = np.concatenate((qstar_dat, qDM_model, qSMBH), axis=None)                           \n",
    "    Total_q_proj_model = (np.sqrt(Total_q_model**2 - np.cos(inc_model)**2)/np.sin(inc_model))          #New projected axial ratio\n",
    "    mass_profile.MGE_Updt_parameters(Total_Mass_model,Total_sigma_RAD.value, Total_q_proj_model, parsDic['gamma'])       #Update the model\n",
    "    \n",
    "    \n",
    "def Updt_JAM(parsDic):\n",
    "    '''\n",
    "       Update the dynamical mass model\n",
    "    input\n",
    "      parsDic: parameter dictionary {'paraName', value}\n",
    "    '''\n",
    "    surf_DM_model = surf_DM_dat.value*(10**parsDic['log_rho_s'])\n",
    "    qDM_model = np.ones(qDM_dat.shape)*parsDic['qDM']\n",
    "    beta_model = np.array(parsDic['beta'])\n",
    "    mbh_model = 10**parsDic['log_mbh']\n",
    "    \n",
    "    \n",
    "    Jampy_Model.Updt_parameters(surf_DM=surf_DM_model, qobs_DM=qDM_model,\n",
    "                                beta=beta_model, ml=np.array(parsDic['ml']),\n",
    "                                inc=parsDic['inc'],mbh=10**parsDic['log_mbh'])\n",
    "\n",
    "def JAM_log_likelihood(parsDic):\n",
    "    \n",
    "    Updt_JAM(parsDic)\n",
    "    \n",
    "    rmsModel, ml, chi2, chi2T = Jampy_Model.run()\n",
    "    #print('Jampy', -0.5 * chi2T )\n",
    "    return -0.5 * chi2T\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def Pyautolens_log_likelihood(parsDic):\n",
    "    \n",
    "    Updt_Pyautolens(parsDic)\n",
    "    \n",
    "    #New lens model\n",
    "    lens_galaxy = al.Galaxy(                                            \n",
    "        redshift=0.035,\n",
    "        mass=mass_profile,\n",
    "        shear=al.mp.ExternalShear(elliptical_comps=(parsDic['mag_shear'], parsDic['phi_shear'])),\n",
    "    )\n",
    "    \n",
    "    \n",
    "    tracer = al.Tracer.from_galaxies(galaxies=[lens_galaxy, al.Galaxy(redshift=2.1)])\n",
    "    source_plane_grid = tracer.traced_grids_of_planes_from_grid(grid=masked_imaging.grid)[1]\n",
    "    \n",
    "    #check if the integral converge. If not, return -np.inf\n",
    "    if np.isnan(source_plane_grid[0,0]):\n",
    "        return -np.inf\n",
    "    \n",
    "    rectangular = al.pix.Rectangular(shape=(50, 50))\n",
    "    mapper = rectangular.mapper_from_grid_and_sparse_grid(grid=source_plane_grid)\n",
    "    \n",
    "    inversion = al.Inversion(\n",
    "        masked_dataset=masked_imaging,\n",
    "        mapper=mapper,\n",
    "        regularization=al.reg.Constant(coefficient=1),\n",
    "    )\n",
    "    chi2T = inversion.chi_squared_map.sum()\n",
    "    #print('Autolens',-0.5 * chi2T )\n",
    "    return -0.5 * chi2T\n",
    "\n",
    "def log_probability(pars):\n",
    "    (m1, m2, m3, m4, m5, m6, b1, b2, b3, b4, b5, b6, b7,\n",
    "         inc, qDM, log_rho_s, log_mbh, mag_shear, phi_shear, gamma) = pars\n",
    "    \n",
    "    ml =  np.array([m1, m1, m2, m3, m4, m4, m6])\n",
    "    beta =  np.array([b1, b2, b3, b4, b5, b6, b7])\n",
    "    parsDic = {'ml': ml, 'inc': inc, 'qDM': qDM, 'log_rho_s': log_rho_s, 'log_mbh': log_mbh,\n",
    "                  'mag_shear': mag_shear, 'phi_shear': phi_shear, 'gamma': gamma, 'beta': beta}\n",
    "    \n",
    "    #Checking boundaries\n",
    "    if not np.isfinite(check_boundary(parsDic)):\n",
    "        return -np.inf\n",
    "    #calculating the log_priors\n",
    "    lp = log_prior(parsDic)\n",
    "    \n",
    "    return lp + Pyautolens_log_likelihood(parsDic) + JAM_log_likelihood(parsDic) \n",
    "    \n",
    "\n",
    "##### For the initial guesses we will use the Collett's best fit with an addition of a random noise between 0 and 1. This allow us probe faster our code.\n",
    "\n",
    "\n",
    "np.savetxt('Output LogFile.txt', np.column_stack([0, 0, 0]),\n",
    "                            fmt=b'\\t%i\\t %e\\t\\t\\t %e\\t ', \n",
    "                            header=\"Output table for the combined model: Lens + Dynamic.\\\\n Iteration\\t Mean acceptance fraction\\t Processing Time\")\n",
    "\n",
    "np.savetxt(\"LogFile_LastFit.txt\", np.column_stack([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,]),\n",
    "                            fmt=b'%e\\t %e\\t %e\\t %e\\t %e\\t %e\\t %e\\t %e\\t %e\\t %e\\t %e\\t %e\\t %e\\t %e\\t %e\\t %e\\t %e\\t %e\\t %e\\t %e\\t %e\\t', \n",
    "                            header=\"Iteration\\t ML1/2\\t ML3\\t ML4\\t ML5\\t ML6\\t ML7\\t b1\\t b2\\t b3\\t b4\\t b5\\t b6\\t b7\\t Inc\\t qDM\\t Logrho_s\\t LogMBH\\t MagShear\\t PhiShear\\t gamma\")\n",
    "\n",
    "\n",
    "with MPIPool() as pool:\n",
    "    if not pool.is_master():\n",
    "        pool.wait()\n",
    "        sys.exit(0)\n",
    "        \n",
    "    np.random.seed(42)\n",
    "    \n",
    "    #Defining initial guesses\n",
    "\n",
    "    ml = np.zeros((50,6))\n",
    "    ml[:] = np.array([9.5,8.5,3.8,3.4,3.2,2.8])\n",
    "    ml_noise = np.random.rand(50,6)\n",
    "\n",
    "    beta = np.zeros((50,7))\n",
    "    beta[:] = np.array([-0.6, -1.0, 0.34, -3.4, 0.39, -0.31, 0.36])\n",
    "    beta_noise = np.random.rand(50,7)\n",
    "\n",
    "    ml = ml + ml_noise                                                               #Between [2.8, 10.5]\n",
    "    beta = beta + beta_noise                                                         #Between [-3.4, 1.39]\n",
    "    inc = prior ['inc'][0] + (np.random.rand(50,1) -0.5)*10                          #Between [85, 95]\n",
    "    qDM = np.random.rand(50,1)*0.74+0.26                                             #Between [0.26, 1]\n",
    "    log_rho_s = np.random.rand(50,1)*prior['log_rho_s'][0]                           #Between [0, 10]\n",
    "    log_mbh =  np.random.rand(50,1)+9.0                                              #Between [9.0, 10]\n",
    "    mag_shear = (np.random.rand(50,1) - 0.5)*0.2                                     #Between [-0.1, 0.1]\n",
    "    phi_shear = (np.random.rand(50,1) - 0.5)*0.2                                     #Between [-0.1, 0.1]\n",
    "    gamma = (np.random.rand(50,1) - 0.5)*2                                           #Between [-1, 1]\n",
    "    #50 walkers in a 21-D space\n",
    "    pos = np.append(ml, beta, axis=1)\n",
    "    pos = np.append(pos, inc, axis=1)\n",
    "    pos = np.append(pos, qDM, axis=1)\n",
    "    pos = np.append(pos, log_rho_s, axis=1)\n",
    "    pos = np.append(pos, log_mbh, axis=1)\n",
    "    pos = np.append(pos, mag_shear, axis=1)\n",
    "    pos = np.append(pos, phi_shear, axis=1)\n",
    "    pos = np.append(pos, gamma, axis=1)\n",
    "\n",
    "    nwalkers, ndim = pos.shape\n",
    "    \n",
    "    # Set up the backend\n",
    "    # Don't forget to clear it in case the file already exists\n",
    "    filename = \"JAMPY+AUTOLENS-CLUSTER.h5\"\n",
    "    backend = emcee.backends.HDFBackend(filename)\n",
    "    backend.reset(nwalkers, ndim)\n",
    "    \n",
    "    # Initialize the sampler\n",
    "    sampler = emcee.EnsembleSampler(nwalkers, ndim, log_probability, backend=backend)\n",
    "    \n",
    "    max_n = 5\n",
    "    \n",
    "    global_time = clock()\n",
    "    # We'll track how the average autocorrelation time estimate changes\n",
    "    index = 0\n",
    "    autocorr = np.empty(max_n)\n",
    "    \n",
    "    # This will be useful to testing convergence\n",
    "    old_tau = np.inf\n",
    "\n",
    "    # Now we'll sample for up to max_n steps\n",
    "    for sample in sampler.sample(pos, iterations=max_n, progress=True):\n",
    "        # Only check convergence every 2 steps\n",
    "        if sampler.iteration % 2:\n",
    "            continue\n",
    "\n",
    "        # Compute the autocorrelation time so far\n",
    "        # Using tol=0 means that we'll always get an estimate even\n",
    "        # if it isn't trustworthy\n",
    "        tau = sampler.get_autocorr_time(tol=0)\n",
    "        autocorr[index] = np.mean(tau)\n",
    "        index += 1\n",
    "\n",
    "\n",
    "\n",
    "        #Update a table output with acceptance\n",
    "        table = np.loadtxt(\"Output LogFile.txt\")\n",
    "\n",
    "\n",
    "        iteration = sampler.iteration\n",
    "        accept = np.mean(sampler.acceptance_fraction)\n",
    "        total_time = clock() - global_time\n",
    "        upt = np.column_stack([iteration, accept, total_time])\n",
    "\n",
    "        np.savetxt('Output LogFile.txt', np.vstack([table, upt]),\n",
    "                                fmt=b'\\t%i\\t %e\\t\\t\\t %e\\t ', \n",
    "                                header=\"Iteration\\t Mean acceptance fraction\\t Processing Time\")\n",
    "\n",
    "        #Update table output with last best fit\n",
    "        last_fit_table = np.loadtxt(\"LogFile_LastFit.txt\")\n",
    "        flat_samples = sampler.get_chain()\n",
    "        values = []\n",
    "        for i in range(ndim):\n",
    "            mcmc = np.percentile(flat_samples[:, i], [16, 50, 84])\n",
    "            q = np.diff(mcmc)\n",
    "            values.append(mcmc[1])\n",
    "\n",
    "        values = np.array(values)\n",
    "        upt = np.append(iteration, values)\n",
    "\n",
    "        np.savetxt(\"LogFile_LastFit.txt\", np.vstack([last_fit_table, upt]),\n",
    "                            fmt=b'%e\\t %e\\t %e\\t %e\\t %e\\t %e\\t %e\\t %e\\t %e\\t %e\\t %e\\t %e\\t %e\\t %e\\t %e\\t %e\\t %e\\t %e\\t %e\\t %e\\t %e\\t', \n",
    "                            header=\"Iteration\\t ML1/2\\t ML3\\t ML4\\t ML5\\t ML6\\t ML7\\t b1\\t b2\\t b3\\t b4\\t b5\\t b6\\t b7\\t Inc\\t qDM\\t Logrho_s\\t LogMBH\\t MagShear\\t PhiShear\\t gamma\")\n",
    " \n",
    "\n",
    "        # Check convergence\n",
    "        converged = np.all(tau * 100 < sampler.iteration)\n",
    "        converged &= np.all(np.abs(old_tau - tau) / tau < 0.01)\n",
    "        if converged:\n",
    "            break\n",
    "        old_tau = tau\n",
    "tau = sampler.get_autocorr_time()\n",
    "print(tau)\n",
    "    \n",
    "    \n",
    "\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = !mpiexec -n {4} python JAM+AUTOLENS_RUN.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '  0%|          | 0/5 [00:00<?, ?it/s]/home/carlos/anaconda3/lib/python3.7/site-packages/autogalaxy/profiles/mass_profiles/total_mass_profiles.py:98: RuntimeWarning: invalid value encountered in sqrt',\n",
       " '  eta = np.sqrt(1 - q**2)',\n",
       " '/home/carlos/anaconda3/lib/python3.7/site-packages/autogalaxy/profiles/mass_profiles/total_mass_profiles.py:120: RuntimeWarning: invalid value encountered in sqrt',\n",
       " '  eta = np.sqrt(1 - q**2)',\n",
       " '/home/carlos/anaconda3/lib/python3.7/site-packages/emcee/moves/red_blue.py:99: RuntimeWarning: invalid value encountered in double_scalars',\n",
       " '  lnpdiff = f + nlp - state.log_prob[j]',\n",
       " '',\n",
       " ' 20%|██        | 1/5 [05:11<20:44, 311.07s/it]',\n",
       " ' 40%|████      | 2/5 [10:49<15:57, 319.30s/it]/home/carlos/anaconda3/lib/python3.7/site-packages/emcee/autocorr.py:36: RuntimeWarning: invalid value encountered in true_divide',\n",
       " '  acf /= acf[0]',\n",
       " '/home/carlos/anaconda3/lib/python3.7/site-packages/emcee/autocorr.py:41: RuntimeWarning: invalid value encountered in less',\n",
       " '  m = np.arange(len(taus)) < c * taus',\n",
       " '/home/carlos/anaconda3/lib/python3.7/site-packages/emcee/autocorr.py:99: RuntimeWarning: invalid value encountered in greater',\n",
       " '  flag = tol * tau_est > n_t',\n",
       " 'JAM+AUTOLENS_RUN.py:595: RuntimeWarning: invalid value encountered in less',\n",
       " '  converged = np.all(tau * 100 < sampler.iteration)',\n",
       " 'JAM+AUTOLENS_RUN.py:596: RuntimeWarning: invalid value encountered in less',\n",
       " '  converged &= np.all(np.abs(old_tau - tau) / tau < 0.01)',\n",
       " '',\n",
       " ' 60%|██████    | 3/5 [16:09<10:38, 319.47s/it]',\n",
       " ' 80%|████████  | 4/5 [23:09<05:49, 349.69s/it]',\n",
       " '100%|██████████| 5/5 [29:46<00:00, 363.96s/it]',\n",
       " '100%|██████████| 5/5 [29:46<00:00, 357.38s/it][nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan',\n",
       " ' nan nan]',\n",
       " '']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import emcee\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "reader = emcee.backends.HDFBackend(\"JAMPY+AUTOLENS-CLUSTER.h5\")\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carlos/anaconda3/lib/python3.7/site-packages/emcee/autocorr.py:36: RuntimeWarning: invalid value encountered in true_divide\n",
      "  acf /= acf[0]\n",
      "/home/carlos/anaconda3/lib/python3.7/site-packages/emcee/autocorr.py:41: RuntimeWarning: invalid value encountered in less\n",
      "  m = np.arange(len(taus)) < c * taus\n",
      "/home/carlos/anaconda3/lib/python3.7/site-packages/emcee/autocorr.py:99: RuntimeWarning: invalid value encountered in greater\n",
      "  flag = tol * tau_est > n_t\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot convert float NaN to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-9509354ee7a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtau\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_autocorr_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mburnin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtau\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mthin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtau\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscard\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mburnin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlog_prob_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscard\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mburnin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot convert float NaN to integer"
     ]
    }
   ],
   "source": [
    "tau = reader.get_autocorr_time()\n",
    "burnin = int(2 * np.max(tau))\n",
    "thin = int(0.5 * np.min(tau))\n",
    "samples = reader.get_chain(discard=burnin, flat=True, thin=thin)\n",
    "log_prob_samples = reader.get_log_prob(discard=burnin, flat=True, thin=thin)\n",
    "log_prior_samples = reader.get_blobs(discard=burnin, flat=True, thin=thin)\n",
    "\n",
    "print(\"burn-in: {0}\".format(burnin))\n",
    "print(\"thin: {0}\".format(thin))\n",
    "print(\"flat chain shape: {0}\".format(samples.shape))\n",
    "print(\"flat log prob shape: {0}\".format(log_prob_samples.shape))\n",
    "print(\"flat log prior shape: {0}\".format(log_prior_samples.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_samples = reader.get_chain()\n",
    "values = []\n",
    "for i in range(20):\n",
    "    mcmc = np.percentile(flat_samples[:, i], [16, 50, 84])\n",
    "    q = np.diff(mcmc)\n",
    "    values.append(mcmc[1])\n",
    "\n",
    "values = np.array(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml = np.zeros((50,6))\n",
    "ml[:] = np.array([9.5,8.5,3.8,3.4,3.2,2.8])\n",
    "ml_noise = np.random.rand(50,6)\n",
    "\n",
    "beta = np.zeros((50,7))\n",
    "beta[:] = np.array([-0.6, -1.0, 0.34, -3.4, 0.39, -0.31, 0.36])\n",
    "beta_noise = np.random.rand(50,7)\n",
    "\n",
    "ml = ml + ml_noise                                                               #Between [2.8, 10.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9.73936238,  8.9842913 ,  3.8478077 ,  3.67690467,  3.36455282,\n",
       "         2.8111387 ],\n",
       "       [ 9.75109942,  8.74313759,  4.73292683,  3.58117582,  3.48469675,\n",
       "         3.00062025],\n",
       "       [10.28404302,  9.10496436,  4.25897468,  3.797757  ,  3.48139494,\n",
       "         2.97344388],\n",
       "       [ 9.74521847,  9.10717708,  4.56221201,  4.37476168,  3.5283089 ,\n",
       "         2.99843554],\n",
       "       [ 9.95997265,  9.34488564,  3.99290685,  4.24052908,  4.19571855,\n",
       "         2.9597543 ],\n",
       "       [ 9.7341465 ,  8.51499774,  4.55099003,  3.72859576,  3.84868007,\n",
       "         2.91302041],\n",
       "       [ 9.54863308,  9.15944718,  4.57049206,  4.14191972,  3.40314115,\n",
       "         3.64439786],\n",
       "       [ 9.62114172,  9.1569102 ,  4.15279312,  3.77603363,  4.19491975,\n",
       "         3.12380424],\n",
       "       [10.41814962,  8.99770858,  4.756624  ,  3.41266184,  3.4404525 ,\n",
       "         3.54156827],\n",
       "       [ 9.75416725,  9.00572324,  4.66351857,  3.52919121,  4.0561342 ,\n",
       "         2.89929293],\n",
       "       [10.2400776 ,  9.41463867,  4.04653196,  3.55478156,  3.41021602,\n",
       "         3.01975497],\n",
       "       [10.00374469,  8.64376234,  4.01133277,  3.45803639,  3.99427181,\n",
       "         3.13377234],\n",
       "       [ 9.74154501,  8.82573794,  3.87905366,  3.55747098,  3.32789014,\n",
       "         3.44133909],\n",
       "       [10.4195944 ,  8.97055632,  4.05620719,  3.61211927,  3.44776131,\n",
       "         3.17029258],\n",
       "       [10.20123171,  8.92658446,  4.21804947,  3.40989572,  3.75923352,\n",
       "         3.50051038],\n",
       "       [10.13616037,  9.40677851,  4.59168202,  3.54563329,  4.13480733,\n",
       "         3.19790828],\n",
       "       [10.04389637,  8.85408071,  4.11920227,  3.85939615,  3.33201441,\n",
       "         2.83734442],\n",
       "       [ 9.68905264,  9.06184625,  4.61477502,  3.6673807 ,  4.06591097,\n",
       "         3.39750897],\n",
       "       [ 9.65049834,  8.51752205,  4.33379651,  3.98847855,  4.02927552,\n",
       "         3.68111917],\n",
       "       [10.45950352,  8.74983109,  4.0812231 ,  4.07658209,  4.10193737,\n",
       "         3.56580825],\n",
       "       [10.35448048,  9.45209345,  4.15778755,  3.71447582,  4.08147483,\n",
       "         3.7821434 ],\n",
       "       [ 9.5041231 ,  8.98613991,  4.25820269,  4.0336685 ,  3.69934705,\n",
       "         3.02074174],\n",
       "       [ 9.98662161,  9.3083241 ,  3.94371304,  3.82133196,  3.81542943,\n",
       "         3.1958364 ],\n",
       "       [ 9.96861084,  8.62527412,  4.33702635,  3.92222366,  3.46452741,\n",
       "         2.85571936],\n",
       "       [10.11520616,  8.82911849,  4.19070657,  3.89878693,  3.43880818,\n",
       "         3.6628514 ],\n",
       "       [10.1526224 ,  8.62647131,  3.93000644,  3.65217744,  3.28772429,\n",
       "         3.76724874],\n",
       "       [ 9.50111437,  8.99867039,  4.75818186,  4.06339671,  3.99978719,\n",
       "         3.0774911 ],\n",
       "       [10.11971713,  8.58816834,  4.42133383,  3.94909653,  4.01323064,\n",
       "         3.58700655],\n",
       "       [ 9.63173792,  9.37249568,  4.72578144,  3.52724103,  4.13479096,\n",
       "         2.91951617],\n",
       "       [10.3689574 ,  8.55031248,  4.43785351,  3.44121845,  4.18601118,\n",
       "         3.77422762],\n",
       "       [ 9.9498056 ,  8.68102807,  4.53747609,  4.08466339,  3.30102763,\n",
       "         3.4959854 ],\n",
       "       [10.00704251,  9.03026169,  4.05214683,  4.25130787,  3.22374834,\n",
       "         3.69371573],\n",
       "       [10.09576719,  9.43300395,  4.2565752 ,  4.13163151,  3.59132761,\n",
       "         3.51628897],\n",
       "       [10.18052785,  8.52232718,  4.79991035,  3.85669418,  3.48147645,\n",
       "         3.43959097],\n",
       "       [ 9.95683818,  8.78898237,  4.51216262,  3.78182199,  3.84060592,\n",
       "         3.77109516],\n",
       "       [10.23728018,  9.25423907,  4.05052708,  4.28926234,  3.47770557,\n",
       "         3.15134243],\n",
       "       [ 9.93063994,  8.6513529 ,  4.29329154,  3.74866667,  3.48655089,\n",
       "         3.05478844],\n",
       "       [10.35094518,  9.45058802,  4.60608971,  3.83387991,  3.76810414,\n",
       "         3.71164459],\n",
       "       [10.40470435,  8.82138705,  3.84952416,  3.95184296,  4.16983704,\n",
       "         2.96367198],\n",
       "       [ 9.93000867,  8.61469611,  4.10654989,  3.87054266,  3.21269639,\n",
       "         3.1923117 ],\n",
       "       [ 9.69136546,  8.68026567,  4.37198987,  3.45386696,  3.220605  ,\n",
       "         3.07307734],\n",
       "       [ 9.55643267,  9.06522267,  4.57725601,  4.33734965,  4.00623269,\n",
       "         3.52889065],\n",
       "       [10.49196974,  8.86633178,  4.59869274,  3.40752912,  3.25856042,\n",
       "         3.62088921],\n",
       "       [ 9.66501066,  8.79317695,  4.77738168,  3.93641457,  4.04659181,\n",
       "         3.59916791],\n",
       "       [10.43690397,  9.03557861,  4.21090625,  3.58271152,  3.41455427,\n",
       "         3.02713423],\n",
       "       [10.11558975,  9.03468839,  3.98857184,  4.01716892,  3.43009352,\n",
       "         3.74947947],\n",
       "       [10.43552855,  9.41102213,  3.85082214,  3.56517381,  3.34657925,\n",
       "         2.96515115],\n",
       "       [10.25281545,  8.71374502,  4.06149478,  3.8055956 ,  3.32236773,\n",
       "         2.83732703],\n",
       "       [ 9.88466083,  9.4594602 ,  4.31053464,  3.97155754,  3.54182934,\n",
       "         3.33903308],\n",
       "       [10.2085127 ,  9.28713089,  3.97372654,  3.62204421,  3.74844199,\n",
       "         3.61007139]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
